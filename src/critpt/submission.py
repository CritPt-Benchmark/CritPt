"""
Submission format for generated solutions.
This module defines the standard format for submitting problem solutions.

CLI Usage:
    # Offline evaluation of a directory of submissions
    python -m critpt.submission <results_dir> --server-url http://localhost:8000

    # Evaluate a single submission file
    python -m critpt.submission <submission.json> --server-url http://localhost:8000
"""
import json
import argparse
import sys
import logging
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List

# Disable verbose httpx/httpcore logging
logging.getLogger("httpx").setLevel(logging.WARNING)
logging.getLogger("httpcore").setLevel(logging.WARNING)

try:
    from tqdm.auto import tqdm
except Exception:
    tqdm = None

@dataclass
class Submission:
    """
    Standardized submission format for a single problem solution.

    This is the format that will be output by the public generation code
    and accepted by the private evaluation server.
    """

    # Problem identification
    problem_id: str  # e.g., "EC_quantum_main" or "EC_quantum_sub_0"

    # Generated solution
    generated_code: str  # The code generated by the model

    # Generation metadata
    model: str  # e.g., "openai/gpt-4"
    timestamp: str  # ISO format timestamp
    generation_config: Dict[str, Any]  # Configuration used for generation

    # Optional: full conversation for debugging
    messages: Optional[List[Dict[str, Any]]] = None

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return asdict(self)

    def to_json(self, file_path: Path) -> None:
        """Save submission to JSON file."""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'Submission':
        """Create submission from dictionary."""
        return cls(**data)

    @classmethod
    def from_json(cls, file_path: Path) -> 'Submission':
        """Load submission from JSON file."""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return cls.from_dict(data)


@dataclass
class SubmissionBatch:
    """
    Batch of submissions for multiple problems.

    This allows submitting solutions for an entire problem set at once.
    """

    submissions: List[Submission]
    batch_metadata: Dict[str, Any]  # Overall batch info (model, config, etc.)

    def to_dict(self) -> Dict[str, Any]:
        """Convert to dictionary."""
        return {
            "submissions": [sub.to_dict() for sub in self.submissions],
            "batch_metadata": self.batch_metadata,
        }

    def to_json(self, file_path: Path) -> None:
        """Save batch to JSON file."""
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(self.to_dict(), f, indent=2, ensure_ascii=False)

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> 'SubmissionBatch':
        """Create batch from dictionary."""
        submissions = [Submission.from_dict(sub) for sub in data["submissions"]]
        return cls(
            submissions=submissions,
            batch_metadata=data["batch_metadata"]
        )

    @classmethod
    def from_json(cls, file_path: Path) -> 'SubmissionBatch':
        """Load batch from JSON file."""
        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        return cls.from_dict(data)

    @classmethod
    def from_directory(cls, directory: Path) -> 'SubmissionBatch':
        """
        Load all submission JSON files from a directory.

        Expected structure:
            directory/
                EC_quantum_main.json
                EC_quantum_sub_0.json
                EC_quantum_sub_1.json
                ...
        """
        submissions = []
        for json_file in directory.glob("*.json"):
            # Skip batch metadata file and main.json (legacy format)
            if json_file.name in ["batch_metadata.json", "main.json"] + [f"sub_{i}.json" for i in range(10)] or "_eval.json" in json_file.name:
                continue
            try:
                submission = Submission.from_json(json_file)
                submissions.append(submission)
            except Exception as e:
                print(f"Warning: Failed to load {json_file}: {e}")

        # Try to load batch metadata
        metadata_file = directory / "batch_metadata.json"
        if metadata_file.exists():
            with open(metadata_file, 'r', encoding='utf-8') as f:
                batch_metadata = json.load(f)
        else:
            batch_metadata = {"directory": str(directory)}

        return cls(submissions=submissions, batch_metadata=batch_metadata)


def create_submission(
    problem_id: str,
    generated_code: str,
    model: str,
    generation_config: Dict[str, Any],
    messages: Optional[List[Dict[str, Any]]] = None
) -> Submission:
    """
    Helper function to create a submission.

    Args:
        problem_id: Unique problem identifier
        generated_code: Generated solution code
        model: Model identifier (e.g., "openai/gpt-4")
        generation_config: Configuration dict
        messages: Optional full conversation history

    Returns:
        Submission object
    """
    return Submission(
        problem_id=problem_id,
        generated_code=generated_code,
        model=model,
        timestamp=datetime.now().isoformat(),
        generation_config=generation_config,
        messages=messages
    )


# async def evaluate_offline_async(
#     path: Path,
#     server_url: str,
#     output_dir: Optional[Path] = None,
#     verbose: bool = True,
#     max_concurrent: int = 10,
#     skip_if_exists: bool = False
# ) -> Dict[str, Any]:
#     """
#     Evaluate submissions offline by sending them to the evaluation server (ASYNC version).

#     Args:
#         path: Path to submission file or directory containing submissions
#         server_url: URL of the evaluation server
#         output_dir: Optional directory to save evaluation results
#         verbose: Print progress messages
#         max_concurrent: Maximum number of concurrent evaluations (default: 10)
#         skip_if_exists: Skip evaluation if result file already exists (default: False)

#     Returns:
#         Dictionary with evaluation results and summary
#     """
#     from critpt.evaluation.eval_client import AsyncEvaluationClient

#     import asyncio

#     path = Path(path)

#     # Load submissions
#     if path.is_file():
#         if verbose:
#             print(f"Loading single submission from: {path}")
#         submissions = [Submission.from_json(path)]
#         batch_metadata = {"source": str(path)}
#     elif path.is_dir():
#         if verbose:
#             print(f"Loading submissions from directory: {path}")
#         batch = SubmissionBatch.from_directory(path)
#         submissions = batch.submissions
#         batch_metadata = batch.batch_metadata
#         if verbose:
#             print(f"Loaded {len(submissions)} submissions")
#     else:
#         print(f"Error: Path does not exist: {path}")
#         sys.exit(1)

#     if len(submissions) == 0:
#         print("Error: No submissions found")
#         sys.exit(1)

#     # Connect to evaluation server
#     if verbose:
#         print(f"\nConnecting to evaluation server: {server_url}")

#     async with AsyncEvaluationClient(server_url) as client:
#         server_mode = "UNKNOWN"
#         # Check server health
#         try:
#             health = await client.check_health()
#             if not health.get('initialized'):
#                 print("Warning: Server is not initialized. Call /init endpoint first.")
#                 print("You may need to initialize the server with golden data.")
#             server_mode = str(health.get('mode', 'unknown')).upper()
#         except Exception as e:
#             print(f"Error: Cannot connect to server: {e}")
#             sys.exit(1)

#         # Always use batch mode (works for 1+ submissions)
#         # PRIVACY/PUBLIC modes require batch endpoints
#         # Batch mode is also faster for multiple submissions
#         if verbose:
#             print(f"\nEvaluating {len(submissions)} submission(s)...")

#         stream_progress = verbose
#         progress_bar = None
#         progress_callback = None

#         if stream_progress:
#             progress_log_step = max(1, len(submissions) // 10)
#             last_logged = 0

#             if tqdm is not None:
#                 progress_bar = tqdm(total=len(submissions), desc="Evaluating", unit="submission", leave=True)

#             def _progress_callback(completed: int, total: int) -> None:
#                 nonlocal progress_log_step, last_logged
#                 if total > 0:
#                     progress_log_step = max(1, total // 10)
#                 if progress_bar is not None:
#                     if total:
#                         progress_bar.total = total
#                     progress_bar.n = completed
#                     progress_bar.refresh()
#                 elif total and (completed == total or completed - last_logged >= progress_log_step):
#                     pct = (completed / total * 100) if total else 0.0
#                     print(f"  Progress: {completed}/{total} ({pct:.1f}%)")
#                     last_logged = completed

#             progress_callback = _progress_callback

#         # Use batch endpoint
#         try:
#             batch_payload = await client.evaluate_batch(
#                 submissions,
#                 batch_metadata,
#                 stream_progress=stream_progress,
#                 progress_callback=progress_callback
#             )
#             if progress_bar is not None:
#                 progress_bar.n = progress_bar.total
#                 progress_bar.refresh()
#                 progress_bar.close()
#             if verbose:
#                 print("\nBatch evaluation complete!")
#         except Exception as e:
#             if progress_bar is not None:
#                 progress_bar.close()
#             print(f"Batch evaluation failed: {e}")
#             sys.exit(1)

#     # Interpret response payload
#     response_mode = (server_mode or "UNKNOWN").upper()
#     metrics = batch_payload

#     if not isinstance(metrics, dict):
#         metrics = {}


#     total = len(submissions)
#     summary = {
#         "mode": response_mode,
#         "total_submissions": total,
#         "accuracy": metrics.get("accuracy", 0.0),
#         "timeout_rate": metrics.get("timeout_rate", 0.0),
#         "server_timeout_count": metrics.get("server_timeout_count", 0)
#     }
#     result_dict = {}

#     if verbose:
#         print("\n" + "=" * 60)
#         print("EVALUATION SUMMARY")
#         print("=" * 60)

#         print(f"Total submissions: {summary['total_submissions']}")
#         print(f"Accuracy: {summary['accuracy']:.2%}")
#         print(f"Timeout rate: {summary['timeout_rate']:.2%}")
#         print(f"Server timeouts: {summary['server_timeout_count']}")

#         print("=" * 60)

#     # Save results if output directory specified
#     if output_dir:
#         # Save summary with individual results
#         summary_file = output_dir / "evaluation_summary.json"
#         full_report = {
#             "summary": summary,
#             "metrics": metrics,
#             "batch_metadata": batch_metadata,
#             "results": result_dict
#         }
#         with open(summary_file, 'w', encoding='utf-8') as f:
#             json.dump(full_report, f, indent=2, ensure_ascii=False)

#         if verbose:
#             print(f"\nResults saved to: {output_dir}")

#     return {
#         "summary": summary,
#         "metrics": metrics,
#         "results": result_dict,
#         "batch_metadata": batch_metadata
#     }


# def evaluate_offline(
#     path: Path,
#     server_url: str,
#     output_dir: Optional[Path] = None,
#     verbose: bool = True
# ) -> Dict[str, Any]:
#     """
#     Evaluate submissions offline by sending them to the evaluation server.

#     Args:
#         path: Path to submission file or directory containing submissions
#         server_url: URL of the evaluation server
#         output_dir: Optional directory to save evaluation results
#         verbose: Print progress messages

#     Returns:
#         Dictionary with evaluation results and summary
#     """
#     try:
#         from critpt.evaluation.eval_client import EvaluationClient
#     except ImportError:
#         print("Error: eval_client not available. Please install httpx: pip install httpx")
#         sys.exit(1)

#     path = Path(path)

#     # Load submissions
#     if path.is_file():
#         if verbose:
#             print(f"Loading single submission from: {path}")
#         submissions = [Submission.from_json(path)]
#         batch_metadata = {"source": str(path)}
#     elif path.is_dir():
#         if verbose:
#             print(f"Loading submissions from directory: {path}")
#         batch = SubmissionBatch.from_directory(path)
#         submissions = batch.submissions
#         batch_metadata = batch.batch_metadata
#         if verbose:
#             print(f"Loaded {len(submissions)} submissions")
#     else:
#         print(f"Error: Path does not exist: {path}")
#         sys.exit(1)

#     if len(submissions) == 0:
#         print("Error: No submissions found")
#         sys.exit(1)

#     # Connect to evaluation server
#     if verbose:
#         print(f"\nConnecting to evaluation server: {server_url}")

#     with EvaluationClient(server_url) as client:
#         server_mode = "UNKNOWN"
#         # Check server health
#         try:
#             health = client.check_health()
#             if not health.get('initialized'):
#                 print("Warning: Server is not initialized. Call /init endpoint first.")
#                 print("You may need to initialize the server with golden data.")
#             server_mode = str(health.get('mode', 'unknown')).upper()
#         except Exception as e:
#             print(f"Error: Cannot connect to server: {e}")
#             sys.exit(1)

#         # Always use batch mode (works for 1+ submissions)
#         # PRIVACY/PUBLIC modes require batch endpoints

#         # Evaluate submissions
#         if verbose:
#             print(f"\nEvaluating {len(submissions)} submission(s)...")

#         # Use batch endpoint
#         if verbose:
#             print(f"\nEvaluating {len(submissions)} submission(s) in batch mode...")

#         stream_progress = verbose
#         progress_bar = None
#         progress_callback = None

#         if stream_progress:
#             progress_log_step = max(1, len(submissions) // 10)
#             last_logged = 0

#             if tqdm is not None:
#                 progress_bar = tqdm(total=len(submissions), desc="Evaluating", unit="submission", leave=True)

#             def _progress_callback(completed: int, total: int) -> None:
#                 nonlocal progress_log_step, last_logged
#                 if total > 0:
#                     progress_log_step = max(1, total // 10)
#                 if progress_bar is not None:
#                     if total:
#                         progress_bar.total = total
#                     progress_bar.n = completed
#                     progress_bar.refresh()
#                 elif total and (completed == total or completed - last_logged >= progress_log_step):
#                     pct = (completed / total * 100) if total else 0.0
#                     print(f"  Progress: {completed}/{total} ({pct:.1f}%)")
#                     last_logged = completed

#             progress_callback = _progress_callback

#         try:
#             batch_payload = client.evaluate_batch(
#                 submissions,
#                 batch_metadata,
#                 stream_progress=stream_progress,
#                 progress_callback=progress_callback
#             )
#             if progress_bar is not None:
#                 progress_bar.n = progress_bar.total
#                 progress_bar.refresh()
#                 progress_bar.close()
#             if verbose:
#                 print("\nBatch evaluation complete!")
#         except Exception as e:
#             if progress_bar is not None:
#                 progress_bar.close()
#             print(f"Batch evaluation failed: {e}")
#             sys.exit(1)

#     response_mode = (server_mode or "UNKNOWN").upper()
#     metrics = batch_payload

#     if not isinstance(metrics, dict):
#         metrics = {}


#     total = len(submissions)
#     summary = {
#         "mode": response_mode,
#         "total_submissions": total,
#         "accuracy": metrics.get("accuracy", 0.0),
#         "timeout_rate": metrics.get("timeout_rate", 0.0),
#         "server_timeout_count": metrics.get("server_timeout_count", 0)
#     }


#     if verbose:
#         print("\n" + "=" * 60)
#         print("EVALUATION SUMMARY")
#         print("=" * 60)

#         print(f"Total submissions: {summary['total_submissions']}")
#         print(f"Accuracy: {summary['accuracy']:.2%}")
#         print(f"Timeout rate: {summary['timeout_rate']:.2%}")
#         print(f"Server timeouts: {summary['server_timeout_count']}")

#         print("=" * 60)

#     # Save results if output directory specified
#     if output_dir:
#         # Save summary with individual results
#         summary_file = output_dir / "evaluation_summary.json"
#         full_report = {
#             "summary": summary,
#             "metrics": metrics,
#             "batch_metadata": batch_metadata
#         }
#         with open(summary_file, 'w', encoding='utf-8') as f:
#             json.dump(full_report, f, indent=2, ensure_ascii=False)

#         if verbose:
#             print(f"\nResults saved to: {output_dir}")

#     return {
#         "summary": summary,
#         "metrics": metrics,
#         "results": result_dict,
#         "batch_metadata": batch_metadata
#     }


def main():
    """CLI entry point for offline evaluation."""
    parser = argparse.ArgumentParser(
        description="Evaluate submissions offline using the evaluation server"
    )
    parser.add_argument(
        "path",
        type=str,
        help="Path to submission JSON file or directory containing submissions"
    )
    parser.add_argument(
        "--server-url",
        type=str,
        default="http://localhost:8000",
        help="URL of the evaluation server (default: http://localhost:8000)"
    )
    parser.add_argument(
        "--output-dir",
        type=str,
        default=None,
        help="Directory to save evaluation results (optional)"
    )
    parser.add_argument(
        "--quiet",
        action="store_true",
        help="Suppress progress messages"
    )

    args = parser.parse_args()

    try:
        evaluate_offline(
            path=Path(args.path),
            server_url=args.server_url,
            output_dir=Path(args.output_dir) if args.output_dir else None,
            verbose=not args.quiet
        )
    except KeyboardInterrupt:
        print("\n\nEvaluation cancelled by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nError: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
